{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6590a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# modeling\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m     15\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m500\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# numericalization\n",
    "from collections import Counter\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "title_fontsize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae382b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d628f2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>ticker</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "      <th>Lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:57</td>\n",
       "      <td>lx21 made 10008 aapl check out learn howtotrad...</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['lx21', 'made', '10008', 'aapl', 'check', 'ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 00:01:36</td>\n",
       "      <td>insanity today weirdo massive selling aapl bid...</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.8271</td>\n",
       "      <td>negative</td>\n",
       "      <td>['insanity', 'today', 'weirdo', 'massive', 'se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 00:01:50</td>\n",
       "      <td>sp100 stocks performance hd low sbux tgt dvn i...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-0.4278</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['sp100', 'stock', 'performance', 'hd', 'low',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 00:06:47</td>\n",
       "      <td>gm tsla volkswagen pushes 2014 record recall t...</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['gm', 'tsla', 'volkswagen', 'push', '2014', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 00:10:05</td>\n",
       "      <td>swing trading 891 return 14 days swingtrading ...</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['swing', 'trading', '891', 'return', '14', 'd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date                                            Content  \\\n",
       "0 2015-01-01 00:00:57  lx21 made 10008 aapl check out learn howtotrad...   \n",
       "1 2015-01-01 00:01:36  insanity today weirdo massive selling aapl bid...   \n",
       "2 2015-01-01 00:01:50  sp100 stocks performance hd low sbux tgt dvn i...   \n",
       "3 2015-01-01 00:06:47  gm tsla volkswagen pushes 2014 record recall t...   \n",
       "4 2015-01-01 00:10:05  swing trading 891 return 14 days swingtrading ...   \n",
       "\n",
       "  ticker   score     label                                  Lemmatized_tokens  \n",
       "0   AAPL  0.0000   neutral  ['lx21', 'made', '10008', 'aapl', 'check', 'ou...  \n",
       "1   AAPL -0.8271  negative  ['insanity', 'today', 'weirdo', 'massive', 'se...  \n",
       "2   AMZN -0.4278   neutral  ['sp100', 'stock', 'performance', 'hd', 'low',...  \n",
       "3   TSLA  0.0000   neutral  ['gm', 'tsla', 'volkswagen', 'push', '2014', '...  \n",
       "4   AAPL  0.0000   neutral  ['swing', 'trading', '891', 'return', '14', 'd...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('./data/tweets_cleaned_tokenized_words.csv', \n",
    "                     parse_dates=['Date'], index_col=['Unnamed: 0'])\n",
    "# tweets.drop(['Content'], axis=1, inplace = True)\n",
    "stocks = pd.read_csv('./data/scraped_stock_2015_2022.csv')\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d19a02",
   "metadata": {},
   "source": [
    "### Numeric Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da991d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all tweets in column\n",
    "all_tweets = tweets['Content'].tolist()\n",
    "\n",
    "# join the tweets togaether to make one big blob\n",
    "text = ' '.join(all_tweets)\n",
    "\n",
    "# finding the most frequent words\n",
    "words = text.split()\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# sort the words so the most frequent are on top\n",
    "total_words = len(words)\n",
    "sorted_words = word_counts.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763c70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an int-mapping dictionary so most common words will have lower indexes\n",
    "vocab_to_int = {w:i+1 for i, (w, c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c69aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1067899"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93f0821d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>ticker</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "      <th>Lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3923954</th>\n",
       "      <td>2022-09-29 22:40:47</td>\n",
       "      <td>group lawmakers led sen elizabeth warren dmass...</td>\n",
       "      <td>PG</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['group', 'lawmaker', 'led', 'sen', 'elizabeth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923955</th>\n",
       "      <td>2022-09-29 22:23:54</td>\n",
       "      <td>nio im money mean bad investment whole market ...</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['nio', 'im', 'money', 'mean', 'bad', 'investm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923956</th>\n",
       "      <td>2022-09-29 18:34:51</td>\n",
       "      <td>today’s drop spx perfect example happens aapl ...</td>\n",
       "      <td>PG</td>\n",
       "      <td>-0.6197</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['today’s', 'drop', 'spx', 'perfect', 'example...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923957</th>\n",
       "      <td>2022-09-29 13:38:47</td>\n",
       "      <td>wage inflation ⬆️ profit margin ⬇️ amzn</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>positive</td>\n",
       "      <td>['wage', 'inflation', '⬆️', 'profit', 'margin'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923958</th>\n",
       "      <td>2022-09-29 13:04:39</td>\n",
       "      <td>amazon loan 150m small businesses next 3 years...</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>positive</td>\n",
       "      <td>['amazon', 'loan', '150m', 'small', 'business'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988231</th>\n",
       "      <td>2016-07-16 13:15:41</td>\n",
       "      <td>reanalysis study square pie charts 2009 – eage...</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['reanalysis', 'study', 'square', 'pie', 'char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988232</th>\n",
       "      <td>2016-07-16 13:05:27</td>\n",
       "      <td>square pie chart beats rest perception study  ...</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['square', 'pie', 'chart', 'beat', 'rest', 'pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988233</th>\n",
       "      <td>2016-07-16 02:35:31</td>\n",
       "      <td>hard ignore mcdonalds obese  futurity</td>\n",
       "      <td>PG</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['hard', 'ignore', 'mcdonalds', 'obese', 'futu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988234</th>\n",
       "      <td>2016-07-15 23:57:39</td>\n",
       "      <td>tamir rice story make police shooting disappea...</td>\n",
       "      <td>PG</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['tamir', 'rice', 'story', 'make', 'police', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988235</th>\n",
       "      <td>2016-07-15 23:41:43</td>\n",
       "      <td>great listeners actually</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>positive</td>\n",
       "      <td>['great', 'listener', 'actually']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18789 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date  \\\n",
       "3923954 2022-09-29 22:40:47   \n",
       "3923955 2022-09-29 22:23:54   \n",
       "3923956 2022-09-29 18:34:51   \n",
       "3923957 2022-09-29 13:38:47   \n",
       "3923958 2022-09-29 13:04:39   \n",
       "...                     ...   \n",
       "3988231 2016-07-16 13:15:41   \n",
       "3988232 2016-07-16 13:05:27   \n",
       "3988233 2016-07-16 02:35:31   \n",
       "3988234 2016-07-15 23:57:39   \n",
       "3988235 2016-07-15 23:41:43   \n",
       "\n",
       "                                                   Content ticker   score  \\\n",
       "3923954  group lawmakers led sen elizabeth warren dmass...     PG -0.0772   \n",
       "3923955  nio im money mean bad investment whole market ...     PG  0.2500   \n",
       "3923956  today’s drop spx perfect example happens aapl ...     PG -0.6197   \n",
       "3923957            wage inflation ⬆️ profit margin ⬇️ amzn     PG  0.4404   \n",
       "3923958  amazon loan 150m small businesses next 3 years...     PG  0.3470   \n",
       "...                                                    ...    ...     ...   \n",
       "3988231  reanalysis study square pie charts 2009 – eage...     PG  0.0000   \n",
       "3988232  square pie chart beats rest perception study  ...     PG  0.0000   \n",
       "3988233              hard ignore mcdonalds obese  futurity     PG -0.4404   \n",
       "3988234  tamir rice story make police shooting disappea...     PG -0.2263   \n",
       "3988235                           great listeners actually     PG  0.6249   \n",
       "\n",
       "            label                                  Lemmatized_tokens  \n",
       "3923954   neutral  ['group', 'lawmaker', 'led', 'sen', 'elizabeth...  \n",
       "3923955   neutral  ['nio', 'im', 'money', 'mean', 'bad', 'investm...  \n",
       "3923956   neutral  ['today’s', 'drop', 'spx', 'perfect', 'example...  \n",
       "3923957  positive  ['wage', 'inflation', '⬆️', 'profit', 'margin'...  \n",
       "3923958  positive  ['amazon', 'loan', '150m', 'small', 'business'...  \n",
       "...           ...                                                ...  \n",
       "3988231   neutral  ['reanalysis', 'study', 'square', 'pie', 'char...  \n",
       "3988232   neutral  ['square', 'pie', 'chart', 'beat', 'rest', 'pe...  \n",
       "3988233   neutral  ['hard', 'ignore', 'mcdonalds', 'obese', 'futu...  \n",
       "3988234   neutral  ['tamir', 'rice', 'story', 'make', 'police', '...  \n",
       "3988235  positive                  ['great', 'listener', 'actually']  \n",
       "\n",
       "[18789 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2117a609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18789"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7790ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[581, 7737, 1027, 6293, 3121, 4680, 15135, 1172, 1817, 289, 7738, 4140, 984, 3, 6294, 3747, 4681, 256, 446, 1070, 38, 1818], [2320, 187, 121, 426, 95, 2034, 675, 67, 213, 4682, 105, 302, 455, 20, 302, 187, 3416, 219, 2672, 114, 63, 18, 3, 22, 52, 2320], [2499, 868, 1126, 619, 1571, 447, 18, 3, 397, 492, 464, 1212, 985, 20, 464, 503, 3748, 1572, 47, 1650, 1213, 67, 2673, 464]]\n"
     ]
    }
   ],
   "source": [
    "# numericalize the words so they are defined by integers rather than strings\n",
    "tweets_int = []\n",
    "for word in all_tweets:\n",
    "    r = [vocab_to_int[w] for w in word.split()]\n",
    "    tweets_int.append(r)\n",
    "print(tweets_int[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d305d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalize the labels so that we can define them as positive, neutral, or negative\n",
    "\n",
    "encoded_labels = [1 if label =='positive' else 0 if label == 'neutral' else -1 for label in tweets['label']]\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "\n",
    "tweets['label'] = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa91166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(tweets_int, seq_length):\n",
    "    ''' Return features of tweets_ints, where each tweet is \n",
    "    padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(tweets_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, twt in enumerate(tweets_int):\n",
    "        tweets_len = len(twt)\n",
    "        \n",
    "        if tweets_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-tweets_len))\n",
    "            new = zeroes+twt\n",
    "        elif tweets_len > seq_length:\n",
    "            new = twt[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "373858ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>ticker</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "      <th>Lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3923954</th>\n",
       "      <td>2022-09-29 22:40:47</td>\n",
       "      <td>group lawmakers led sen elizabeth warren dmass...</td>\n",
       "      <td>PG</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>0</td>\n",
       "      <td>['group', 'lawmaker', 'led', 'sen', 'elizabeth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923955</th>\n",
       "      <td>2022-09-29 22:23:54</td>\n",
       "      <td>nio im money mean bad investment whole market ...</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>['nio', 'im', 'money', 'mean', 'bad', 'investm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923956</th>\n",
       "      <td>2022-09-29 18:34:51</td>\n",
       "      <td>today’s drop spx perfect example happens aapl ...</td>\n",
       "      <td>PG</td>\n",
       "      <td>-0.6197</td>\n",
       "      <td>0</td>\n",
       "      <td>['today’s', 'drop', 'spx', 'perfect', 'example...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923957</th>\n",
       "      <td>2022-09-29 13:38:47</td>\n",
       "      <td>wage inflation ⬆️ profit margin ⬇️ amzn</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>1</td>\n",
       "      <td>['wage', 'inflation', '⬆️', 'profit', 'margin'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923958</th>\n",
       "      <td>2022-09-29 13:04:39</td>\n",
       "      <td>amazon loan 150m small businesses next 3 years...</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>1</td>\n",
       "      <td>['amazon', 'loan', '150m', 'small', 'business'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988231</th>\n",
       "      <td>2016-07-16 13:15:41</td>\n",
       "      <td>reanalysis study square pie charts 2009 – eage...</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>['reanalysis', 'study', 'square', 'pie', 'char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988232</th>\n",
       "      <td>2016-07-16 13:05:27</td>\n",
       "      <td>square pie chart beats rest perception study  ...</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>['square', 'pie', 'chart', 'beat', 'rest', 'pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988233</th>\n",
       "      <td>2016-07-16 02:35:31</td>\n",
       "      <td>hard ignore mcdonalds obese  futurity</td>\n",
       "      <td>PG</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>0</td>\n",
       "      <td>['hard', 'ignore', 'mcdonalds', 'obese', 'futu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988234</th>\n",
       "      <td>2016-07-15 23:57:39</td>\n",
       "      <td>tamir rice story make police shooting disappea...</td>\n",
       "      <td>PG</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>0</td>\n",
       "      <td>['tamir', 'rice', 'story', 'make', 'police', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988235</th>\n",
       "      <td>2016-07-15 23:41:43</td>\n",
       "      <td>great listeners actually</td>\n",
       "      <td>PG</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>1</td>\n",
       "      <td>['great', 'listener', 'actually']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18789 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date  \\\n",
       "3923954 2022-09-29 22:40:47   \n",
       "3923955 2022-09-29 22:23:54   \n",
       "3923956 2022-09-29 18:34:51   \n",
       "3923957 2022-09-29 13:38:47   \n",
       "3923958 2022-09-29 13:04:39   \n",
       "...                     ...   \n",
       "3988231 2016-07-16 13:15:41   \n",
       "3988232 2016-07-16 13:05:27   \n",
       "3988233 2016-07-16 02:35:31   \n",
       "3988234 2016-07-15 23:57:39   \n",
       "3988235 2016-07-15 23:41:43   \n",
       "\n",
       "                                                   Content ticker   score  \\\n",
       "3923954  group lawmakers led sen elizabeth warren dmass...     PG -0.0772   \n",
       "3923955  nio im money mean bad investment whole market ...     PG  0.2500   \n",
       "3923956  today’s drop spx perfect example happens aapl ...     PG -0.6197   \n",
       "3923957            wage inflation ⬆️ profit margin ⬇️ amzn     PG  0.4404   \n",
       "3923958  amazon loan 150m small businesses next 3 years...     PG  0.3470   \n",
       "...                                                    ...    ...     ...   \n",
       "3988231  reanalysis study square pie charts 2009 – eage...     PG  0.0000   \n",
       "3988232  square pie chart beats rest perception study  ...     PG  0.0000   \n",
       "3988233              hard ignore mcdonalds obese  futurity     PG -0.4404   \n",
       "3988234  tamir rice story make police shooting disappea...     PG -0.2263   \n",
       "3988235                           great listeners actually     PG  0.6249   \n",
       "\n",
       "         label                                  Lemmatized_tokens  \n",
       "3923954      0  ['group', 'lawmaker', 'led', 'sen', 'elizabeth...  \n",
       "3923955      0  ['nio', 'im', 'money', 'mean', 'bad', 'investm...  \n",
       "3923956      0  ['today’s', 'drop', 'spx', 'perfect', 'example...  \n",
       "3923957      1  ['wage', 'inflation', '⬆️', 'profit', 'margin'...  \n",
       "3923958      1  ['amazon', 'loan', '150m', 'small', 'business'...  \n",
       "...        ...                                                ...  \n",
       "3988231      0  ['reanalysis', 'study', 'square', 'pie', 'char...  \n",
       "3988232      0  ['square', 'pie', 'chart', 'beat', 'rest', 'pe...  \n",
       "3988233      0  ['hard', 'ignore', 'mcdonalds', 'obese', 'futu...  \n",
       "3988234      0  ['tamir', 'rice', 'story', 'make', 'police', '...  \n",
       "3988235      1                  ['great', 'listener', 'actually']  \n",
       "\n",
       "[18789 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba9813a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_len = [len(x) for x in tweets_int]\n",
    "\n",
    "X = pd.DataFrame(pad_features(tweets_int, max(tweets_len)))\n",
    "y = pd.DataFrame(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8766e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X.to_csv('./data/tweets_numericalized.csv')\n",
    "# # Y.to_csv('./data/sentiments_numericalized.csv')\n",
    "\n",
    "# X = pd.read_csv('./data/tweets_numericalized.csv', index_col = ['Unnamed: 0'])\n",
    "# y = pd.read_csv('./data/sentiments_numericalized.csv', index_col = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b9549ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into test-train-validation sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5785b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(X_train.values), torch.from_numpy(y_train.values))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid.values), torch.from_numpy(y_valid.values))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test.values), torch.from_numpy(y_test.values))\n",
    "\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d89ada4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 26])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,   109, 15677, 15678],\n",
      "        [    0,     0,     0,  ...,   217,  1307,   332],\n",
      "        [    0,     0,     0,  ...,    19,   975,  2630],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,   265,  5436,     3],\n",
      "        [    0,     0,     0,  ...,     1,     2,     6],\n",
      "        [    0,     0,     0,  ...,     0,  1893,  3596]])\n",
      "\n",
      "Sample label size:  torch.Size([50, 1])\n",
      "Sample label: \n",
      " tensor([ 1,  0,  1,  0,  1,  0,  1,  0,  0,  0,  1,  0,  1,  1,  1,  0,  0,  0,\n",
      "         0,  0,  0,  1,  0,  1,  0,  0,  1,  0,  0, -1,  1,  0,  1,  1,  0,  0,\n",
      "         0,  1,  1,  0,  0,  0,  1,  1,  0, -1,  0,  0,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d11b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46baaca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(39291, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32d598b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 10... Loss: 0.279779... Val Loss: 0.478185\n",
      "Epoch: 1/4... Step: 20... Loss: 0.568976... Val Loss: 0.464345\n",
      "Epoch: 1/4... Step: 30... Loss: 0.418834... Val Loss: 0.446171\n",
      "Epoch: 1/4... Step: 40... Loss: 0.489964... Val Loss: 0.433322\n",
      "Epoch: 1/4... Step: 50... Loss: 0.389284... Val Loss: 0.417790\n",
      "Epoch: 1/4... Step: 60... Loss: 0.261298... Val Loss: 0.413393\n",
      "Epoch: 1/4... Step: 70... Loss: 0.223266... Val Loss: 0.407062\n",
      "Epoch: 1/4... Step: 80... Loss: 0.586663... Val Loss: 0.416382\n",
      "Epoch: 1/4... Step: 90... Loss: 0.284597... Val Loss: 0.395099\n",
      "Epoch: 1/4... Step: 100... Loss: 0.445391... Val Loss: 0.371032\n",
      "Epoch: 1/4... Step: 110... Loss: 0.503528... Val Loss: 0.380906\n",
      "Epoch: 1/4... Step: 120... Loss: 0.124991... Val Loss: 0.437549\n",
      "Epoch: 1/4... Step: 130... Loss: 0.184448... Val Loss: 0.394775\n",
      "Epoch: 1/4... Step: 140... Loss: 0.493796... Val Loss: 0.336545\n",
      "Epoch: 1/4... Step: 150... Loss: 0.320937... Val Loss: 0.331020\n",
      "Epoch: 1/4... Step: 160... Loss: 0.278308... Val Loss: 0.328724\n",
      "Epoch: 1/4... Step: 170... Loss: 0.410170... Val Loss: 0.322492\n",
      "Epoch: 1/4... Step: 180... Loss: 0.121426... Val Loss: 0.319239\n",
      "Epoch: 1/4... Step: 190... Loss: 0.274264... Val Loss: 0.325181\n",
      "Epoch: 1/4... Step: 200... Loss: 0.207463... Val Loss: 0.295228\n",
      "Epoch: 1/4... Step: 210... Loss: 0.202839... Val Loss: 0.282710\n",
      "Epoch: 1/4... Step: 220... Loss: 0.155962... Val Loss: 0.244654\n",
      "Epoch: 1/4... Step: 230... Loss: 0.313284... Val Loss: 0.262790\n",
      "Epoch: 1/4... Step: 240... Loss: 0.444571... Val Loss: 0.219465\n",
      "Epoch: 2/4... Step: 250... Loss: 0.020813... Val Loss: 0.211036\n",
      "Epoch: 2/4... Step: 260... Loss: 0.333857... Val Loss: 0.184012\n",
      "Epoch: 2/4... Step: 270... Loss: -0.478210... Val Loss: 0.189260\n",
      "Epoch: 2/4... Step: 280... Loss: 1.178840... Val Loss: 3.255366\n",
      "Epoch: 2/4... Step: 290... Loss: 2.783720... Val Loss: 2.874024\n",
      "Epoch: 2/4... Step: 300... Loss: 1.656681... Val Loss: 1.822319\n",
      "Epoch: 2/4... Step: 310... Loss: 1.062080... Val Loss: 0.831919\n",
      "Epoch: 2/4... Step: 320... Loss: 0.599612... Val Loss: 0.536270\n",
      "Epoch: 2/4... Step: 330... Loss: 0.435484... Val Loss: 0.484273\n",
      "Epoch: 2/4... Step: 340... Loss: 0.441704... Val Loss: 0.506801\n",
      "Epoch: 2/4... Step: 350... Loss: 0.408457... Val Loss: 0.484320\n",
      "Epoch: 2/4... Step: 360... Loss: 0.461631... Val Loss: 0.484406\n",
      "Epoch: 2/4... Step: 370... Loss: 0.547255... Val Loss: 0.486088\n",
      "Epoch: 2/4... Step: 380... Loss: 0.496592... Val Loss: 0.483471\n",
      "Epoch: 2/4... Step: 390... Loss: 0.369732... Val Loss: 0.486740\n",
      "Epoch: 2/4... Step: 400... Loss: 0.401518... Val Loss: 0.492369\n",
      "Epoch: 2/4... Step: 410... Loss: 0.444930... Val Loss: 0.484380\n",
      "Epoch: 2/4... Step: 420... Loss: 0.381193... Val Loss: 0.483779\n",
      "Epoch: 2/4... Step: 430... Loss: 0.761353... Val Loss: 0.503154\n",
      "Epoch: 2/4... Step: 440... Loss: 0.637624... Val Loss: 0.484769\n",
      "Epoch: 2/4... Step: 450... Loss: 0.434521... Val Loss: 0.489204\n",
      "Epoch: 2/4... Step: 460... Loss: 0.380400... Val Loss: 0.486450\n",
      "Epoch: 2/4... Step: 470... Loss: 0.358150... Val Loss: 0.483850\n",
      "Epoch: 2/4... Step: 480... Loss: 0.566528... Val Loss: 0.487255\n",
      "Epoch: 3/4... Step: 490... Loss: 0.617511... Val Loss: 0.486466\n",
      "Epoch: 3/4... Step: 500... Loss: 0.515827... Val Loss: 0.484841\n",
      "Epoch: 3/4... Step: 510... Loss: 0.480570... Val Loss: 0.489283\n",
      "Epoch: 3/4... Step: 520... Loss: 0.526743... Val Loss: 0.484217\n",
      "Epoch: 3/4... Step: 530... Loss: 0.375452... Val Loss: 0.487801\n",
      "Epoch: 3/4... Step: 540... Loss: 0.593477... Val Loss: 0.489831\n",
      "Epoch: 3/4... Step: 550... Loss: 0.581967... Val Loss: 0.484233\n",
      "Epoch: 3/4... Step: 560... Loss: 0.408691... Val Loss: 0.486808\n",
      "Epoch: 3/4... Step: 570... Loss: 0.287195... Val Loss: 0.490330\n",
      "Epoch: 3/4... Step: 580... Loss: 0.464008... Val Loss: 0.499232\n",
      "Epoch: 3/4... Step: 590... Loss: 0.189081... Val Loss: 0.482484\n",
      "Epoch: 3/4... Step: 600... Loss: 0.467301... Val Loss: 0.492296\n",
      "Epoch: 3/4... Step: 610... Loss: 0.564680... Val Loss: 0.483206\n",
      "Epoch: 3/4... Step: 620... Loss: 0.558860... Val Loss: 0.485196\n",
      "Epoch: 3/4... Step: 630... Loss: 0.615608... Val Loss: 0.482534\n",
      "Epoch: 3/4... Step: 640... Loss: 0.724969... Val Loss: 0.477631\n",
      "Epoch: 3/4... Step: 650... Loss: 0.387842... Val Loss: 0.471002\n",
      "Epoch: 3/4... Step: 660... Loss: 0.468925... Val Loss: 0.536914\n",
      "Epoch: 3/4... Step: 670... Loss: 0.534330... Val Loss: 0.488726\n",
      "Epoch: 3/4... Step: 680... Loss: 0.443681... Val Loss: 0.499907\n",
      "Epoch: 3/4... Step: 690... Loss: 0.278170... Val Loss: 0.485462\n",
      "Epoch: 3/4... Step: 700... Loss: 0.591691... Val Loss: 0.470879\n",
      "Epoch: 3/4... Step: 710... Loss: 0.401631... Val Loss: 0.411314\n",
      "Epoch: 3/4... Step: 720... Loss: 0.081729... Val Loss: 0.435349\n",
      "Epoch: 4/4... Step: 730... Loss: 0.307784... Val Loss: 0.363915\n",
      "Epoch: 4/4... Step: 740... Loss: 0.555058... Val Loss: 0.461635\n",
      "Epoch: 4/4... Step: 750... Loss: 0.203612... Val Loss: 0.365507\n",
      "Epoch: 4/4... Step: 760... Loss: -0.033041... Val Loss: 0.405650\n",
      "Epoch: 4/4... Step: 770... Loss: 0.293335... Val Loss: 0.324174\n",
      "Epoch: 4/4... Step: 780... Loss: 0.133293... Val Loss: 0.335402\n",
      "Epoch: 4/4... Step: 790... Loss: -0.229763... Val Loss: 0.355068\n",
      "Epoch: 4/4... Step: 800... Loss: 0.165965... Val Loss: 0.302625\n",
      "Epoch: 4/4... Step: 810... Loss: 0.175188... Val Loss: 0.286603\n",
      "Epoch: 4/4... Step: 820... Loss: 0.040554... Val Loss: 0.264828\n",
      "Epoch: 4/4... Step: 830... Loss: -0.094611... Val Loss: 0.328573\n",
      "Epoch: 4/4... Step: 840... Loss: 0.036464... Val Loss: 0.567457\n",
      "Epoch: 4/4... Step: 850... Loss: -0.143516... Val Loss: 0.296319\n",
      "Epoch: 4/4... Step: 860... Loss: -0.217841... Val Loss: 0.244249\n",
      "Epoch: 4/4... Step: 870... Loss: 0.418319... Val Loss: 0.239124\n",
      "Epoch: 4/4... Step: 880... Loss: -0.113147... Val Loss: 0.195146\n",
      "Epoch: 4/4... Step: 890... Loss: 0.029919... Val Loss: 0.174442\n",
      "Epoch: 4/4... Step: 900... Loss: -0.413266... Val Loss: 0.213180\n",
      "Epoch: 4/4... Step: 910... Loss: -0.449334... Val Loss: 0.183182\n",
      "Epoch: 4/4... Step: 920... Loss: -0.141215... Val Loss: 0.160740\n",
      "Epoch: 4/4... Step: 930... Loss: -0.281779... Val Loss: 0.180868\n",
      "Epoch: 4/4... Step: 940... Loss: -0.143466... Val Loss: 0.134297\n",
      "Epoch: 4/4... Step: 950... Loss: -0.729315... Val Loss: 0.118691\n",
      "Epoch: 4/4... Step: 960... Loss: -0.083190... Val Loss: 0.051235\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = False\n",
    "\n",
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 10\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        labels = labels.squeeze()\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        output = output.squeeze()\n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                labels = labels.squeeze()\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                inputs = inputs.type(torch.LongTensor)\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(f\"Epoch: {e+1}/{epochs}...\",\n",
    "                  f\"Step: {counter}...\",\n",
    "                  f\"Loss: {loss.item():.6f}...\",\n",
    "                  f\"Val Loss: {np.mean(val_losses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49ed04e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.003\n",
      "Test accuracy: 0.770\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "    \n",
    "    labels = labels.squeeze()\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(f\"Test loss: {np.mean(test_losses):.3f}\")\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edf41852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.265084832906723,\n",
       " -0.7018386721611023,\n",
       " 0.6793506145477295,\n",
       " -0.8892024159431458,\n",
       " 0.4299418330192566,\n",
       " 0.15710344910621643,\n",
       " 1.121410846710205,\n",
       " 0.02134595438838005,\n",
       " 0.45937344431877136,\n",
       " -0.2865404188632965,\n",
       " 0.7863173484802246,\n",
       " -0.49968934059143066,\n",
       " 0.4674068093299866,\n",
       " -0.082013338804245,\n",
       " -0.10906907916069031,\n",
       " -0.011358356103301048,\n",
       " -0.18553072214126587,\n",
       " -1.7107675075531006,\n",
       " -0.3410903811454773,\n",
       " 0.0731324851512909,\n",
       " 0.617652952671051,\n",
       " 0.33814966678619385,\n",
       " 0.6645143032073975,\n",
       " 0.7397577166557312,\n",
       " -0.8421439528465271,\n",
       " -0.030098600313067436,\n",
       " 0.30411165952682495,\n",
       " -0.06867992877960205,\n",
       " 0.2350694239139557,\n",
       " 0.12331392616033554,\n",
       " 0.22327089309692383,\n",
       " -0.6173611879348755,\n",
       " 0.14676427841186523,\n",
       " -0.20221295952796936,\n",
       " 0.008613292127847672,\n",
       " 0.18475717306137085,\n",
       " -0.34524184465408325,\n",
       " 0.3647984266281128,\n",
       " -0.9183323383331299,\n",
       " -0.3468364477157593,\n",
       " -0.41568198800086975,\n",
       " 0.26225724816322327,\n",
       " -0.23032374680042267,\n",
       " 0.026247955858707428,\n",
       " -0.42285779118537903,\n",
       " 0.738865077495575,\n",
       " 0.31475743651390076,\n",
       " 0.3410348892211914,\n",
       " -0.30681517720222473,\n",
       " 0.10632429271936417,\n",
       " -0.985663652420044,\n",
       " -0.41887137293815613,\n",
       " 0.031456079334020615,\n",
       " 0.4590147137641907,\n",
       " 1.2340590953826904,\n",
       " -1.1842588186264038,\n",
       " -0.4492309093475342,\n",
       " 0.3592361509799957,\n",
       " -0.6862860321998596,\n",
       " 0.01242888905107975,\n",
       " 0.1907874494791031,\n",
       " -0.12100119888782501,\n",
       " 0.8944566249847412,\n",
       " 0.15601450204849243,\n",
       " -0.083367258310318,\n",
       " 0.34120017290115356,\n",
       " 0.6777624487876892,\n",
       " 0.13748173415660858,\n",
       " -0.4306167960166931,\n",
       " -0.24551841616630554,\n",
       " -0.02898387983441353,\n",
       " 0.27133241295814514,\n",
       " 0.3887239694595337,\n",
       " -0.5250402688980103,\n",
       " -0.41373834013938904]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a4eb768f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(vocab_to_int.values(), index=vocab_to_int.keys())\n",
    "\n",
    "test_df.to_csv('./data/vocab_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3498640",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_review_neg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_ints\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# test code and generate tokenized review\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m test_ints \u001b[38;5;241m=\u001b[39m tokenize_review(\u001b[43mtest_review_neg\u001b[49m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_ints)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# test sequence padding\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_review_neg' is not defined"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    \n",
    "    test_review = test_review.lower() # lowercase\n",
    "    \n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "    \n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(test_review_neg)\n",
    "print(test_ints)\n",
    "\n",
    "\n",
    "# test sequence padding\n",
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "\n",
    "print(features)\n",
    "\n",
    "\n",
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())\n",
    "\n",
    "\n",
    "def predict(net, test_review, sequence_length=26):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print(f'Prediction value, pre-rounding: {output.item():.6f}')\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e58ecc6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tweet \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpredict\u001b[49m(net, tweet, \u001b[38;5;241m26\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "tweet = 'no'\n",
    "predict(net, tweet, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3aa82755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'top tech stocks highest share buybacks past quarter aapl googl msft meta chtr nvda wmt amzn klac v csco'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets['label'] == 1]['Content'].iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9f3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
