{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0iJhpVAb-eB",
    "outputId": "e68869d9-f5d6-4022-f124-c81f3c704c61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/setone/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "# numericalization\n",
    "from collections import Counter\n",
    "\n",
    "# preprocessing\n",
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# neural nets\n",
    "import tensorflow as tf\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras import Sequential, Input, optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "title_fontsize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDgqHbJ6cCUl",
    "outputId": "50d075fa-fa14-4f8c-af6f-4e853a89b54f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBRWhybdC3aY"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/balanced_untokenized_cleaned_stocktwits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyjLAVu-CsSQ"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "corpus = df['body'].values\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YRZRlvFCtzz"
   },
   "outputs": [],
   "source": [
    "# create an int-mapping dictionary\n",
    "vocab_to_int = tokenizer.word_index\n",
    "\n",
    "max_padding = 25\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "padded_sequences = pad_sequences(sequences, max_padding, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUPB8vJcrPYc"
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('/content/drive/MyDrive/padded_X.csv')\n",
    "y = pd.read_csv('/content/drive/MyDrive/padded_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZOe2XrTcKKW"
   },
   "outputs": [],
   "source": [
    "# X = pd.read_csv('/content/drive/MyDrive/padded_X.csv')\n",
    "# y = pd.read_csv('/content/drive/MyDrive/padded_y.csv')\n",
    "\n",
    "# print(X.shape, y.shape)\n",
    "\n",
    "X = pd.DataFrame(padded_sequences)\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQe2zd4ulxeq"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y88Fc4HVcZSZ",
    "outputId": "f2db7ac0-ba0d-4177-edd6-f6f8c2bbb646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 31, 300)           1500000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               160400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,660,501\n",
      "Trainable params: 1,660,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_vector_length = 300\n",
    "model.add(Embedding(5000, embedding_vector_length, input_length=31)) \n",
    "model.add(LSTM(100)) \n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "Adam = optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam, metrics=['accuracy']) \n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJbFJ9_FCnWg",
    "outputId": "3f36958e-f0f2-4afc-edd1-a0a21d765e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "23221/23221 [==============================] - 173s 7ms/step - loss: 0.5118 - accuracy: 0.7376 - val_loss: 0.4794 - val_accuracy: 0.7595\n",
      "Epoch 2/5\n",
      "23221/23221 [==============================] - 168s 7ms/step - loss: 0.4624 - accuracy: 0.7698 - val_loss: 0.4651 - val_accuracy: 0.7688\n",
      "Epoch 3/5\n",
      "23221/23221 [==============================] - 169s 7ms/step - loss: 0.4408 - accuracy: 0.7824 - val_loss: 0.4556 - val_accuracy: 0.7741\n",
      "Epoch 4/5\n",
      "23221/23221 [==============================] - 165s 7ms/step - loss: 0.4228 - accuracy: 0.7933 - val_loss: 0.4548 - val_accuracy: 0.7760\n",
      "Epoch 5/5\n",
      "23221/23221 [==============================] - 166s 7ms/step - loss: 0.4056 - accuracy: 0.8038 - val_loss: 0.4540 - val_accuracy: 0.7762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c06dccb50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVtALMECtK_Q"
   },
   "outputs": [],
   "source": [
    "for lr in [0.0005, 0.0003, 0.0001, 0.009, 0.007, 0.005, 0.003]:\n",
    "  for bs in [1024, 512, 256, 128, 64, 32]:\n",
    "    print(f'batch size: {bs}, learning rate: {lr}')\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_vector_length = 300\n",
    "    model.add(Embedding(5000, embedding_vector_length, input_length=149)) \n",
    "    model.add(LSTM(100)) \n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    Adam = optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam, metrics=['accuracy']) \n",
    "    # model.summary()\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=bs)\n",
    "    # score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    # print(f'     score: {score[0]}, loss: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7lIznrYTwmyy",
    "outputId": "9fdff016-1bc1-4a1c-84aa-353103551128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6176503300666809\n",
      "Test accuracy: 0.7711316347122192\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsFK7tbtnNiQ",
    "outputId": "4f59abef-1dfe-4ec5-be31-32d169e83241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 770ms/step\n",
      "im bullish about this stock [[0.17795417]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "im bearish about this stock [[0.67036855]]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "short this stock [[0.9892418]]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "i just lost all my money [[0.5148862]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "oh crap im broke [[0.10336161]]\n"
     ]
    }
   ],
   "source": [
    "# run preprocess function down below first\n",
    "\n",
    "text = 'im bullish about this stock'\n",
    "print(text, model.predict(pad_sequences(preprocess(text), 31, padding='post')))\n",
    "\n",
    "text = 'im bearish about this stock'\n",
    "print(text, model.predict(pad_sequences(preprocess(text), 31, padding='post')))\n",
    "\n",
    "text = 'short this stock'\n",
    "print(text, model.predict(pad_sequences(preprocess(text), 31, padding='post')))\n",
    "\n",
    "text = 'i just lost all my money'\n",
    "print(text, model.predict(pad_sequences(preprocess(text), 31, padding='post')))\n",
    "\n",
    "text = 'oh crap im broke'\n",
    "print(text, model.predict(pad_sequences(preprocess(text), 31, padding='post')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYy8mBMCOpoN"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('/content/drive/MyDrive/LSTM_model_0005LR.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08i0kMqPl9sY"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/MyDrive/LSTM_model_0005LR.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYvSjia7YdvM",
    "outputId": "9742621a-8c2a-4cb3-c03e-84fc0b99489f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "dct = pd.read_csv('/content/drive/MyDrive/vocab_words.csv').to_dict(orient='records')[0]\n",
    "\n",
    "websites = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)'\n",
    "numbers = '\\d+'\n",
    "usernames = '@[^\\s]+'\n",
    "tickers = '\\$[^\\s]+'\n",
    "extra_spaces = '  +'\n",
    "hashtags = '\\$[^\\s]+'\n",
    "next_lines = '\\\\n'\n",
    "\n",
    "punctuation = \"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~‘’“”1234567890…ðŸ‘‰ðŸ‘ŒðŸ’¦âœ¨✰♡*•˛❤•\" + '\"'\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', \n",
    "             'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \n",
    "             'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', \n",
    "             'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', \n",
    "             'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \n",
    "             'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
    "             'after', 'to', 'from', 'in', 'out', 'on', 'off', 'again', 'further', 'then', 'once', 'here', 'there', \n",
    "             'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', \n",
    "             'such', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', \n",
    "             'don', 'dont', 'should', 'shouldve', 'now', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', \n",
    "             'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', \n",
    "             'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', \n",
    "             'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt', 'v', 'rn', 'lt', 'y', 'g', 'w', \n",
    "             'wk', 'sp', 'em', 'r', 'vs', 'd', 'ai', 't', 'mm', 'st', 'gt', 'n', 'id', 'p', 'f', 'm', 'b', 'c', \n",
    "             'pe', 'th', 'q', 'x', 'fb', 'ah', 'ill', 'u', 'oh', 'er', 'k', 's', 'im']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2Cqn2zTVqR0"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''\n",
    "    preprocess the text to input into model\n",
    "    '''\n",
    "    \n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # make texts lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove websites and usernames, if exist\n",
    "    text = re.sub(websites, '', text)\n",
    "    text = re.sub(usernames, '', text)\n",
    "    text = re.sub(numbers, '', text)\n",
    "    text = re.sub(tickers, '', text)\n",
    "    text = re.sub(hashtags, '', text)\n",
    "    text = re.sub(next_lines, '', text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = ''.join([x for x in text if x not in punctuation])\n",
    "    \n",
    "    # remove additional characters down to 2\n",
    "    text = re.sub(re.compile(r'(\\w)\\1+'), r'\\1\\1', text)\n",
    "    \n",
    "    # remove stop words\n",
    "    text = ' '.join(text.lower() for text in text.split() if text not in stopwords)\n",
    "    \n",
    "    # remove additional spaces\n",
    "    text = re.sub(extra_spaces, '', text)\n",
    "    \n",
    "    # lemmatize & tokenize\n",
    "    text = [lemmatizer.lemmatize(x) for x in w_tokenizer.tokenize(text)]\n",
    "    \n",
    "    # numericalize\n",
    "    text_int = []\n",
    "    text_int.append([dct.get(word, 0) for word in text])\n",
    "    \n",
    "    return text_int"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
