{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0968a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/setone/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/setone/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/setone/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/setone/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "# numericalization\n",
    "from collections import Counter\n",
    "\n",
    "# preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # will give an altered version later cuz the default isn't great\n",
    "from string import punctuation\n",
    "# from textblob import TextBlob\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# neural nets\n",
    "import tensorflow as tf\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras import Sequential, Input, optimizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "title_fontsize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d628f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data/StockTwits_cleaned.csv')\n",
    "df = pd.read_csv('./data/large_datafiles/Tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a44d135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>writer</th>\n",
       "      <th>post_date</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_num</th>\n",
       "      <th>retweet_num</th>\n",
       "      <th>like_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550441509175443456</td>\n",
       "      <td>VisualStockRSRC</td>\n",
       "      <td>1420070457</td>\n",
       "      <td>lx21 made $10,008  on $AAPL -Check it out! htt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550441672312512512</td>\n",
       "      <td>KeralaGuy77</td>\n",
       "      <td>1420070496</td>\n",
       "      <td>Insanity of today weirdo massive selling. $aap...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>550441732014223360</td>\n",
       "      <td>DozenStocks</td>\n",
       "      <td>1420070510</td>\n",
       "      <td>S&amp;P100 #Stocks Performance $HD $LOW $SBUX $TGT...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>550442977802207232</td>\n",
       "      <td>ShowDreamCar</td>\n",
       "      <td>1420070807</td>\n",
       "      <td>$GM $TSLA: Volkswagen Pushes 2014 Record Recal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>550443807834402816</td>\n",
       "      <td>i_Know_First</td>\n",
       "      <td>1420071005</td>\n",
       "      <td>Swing Trading: Up To 8.91% Return In 14 Days h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717959</th>\n",
       "      <td>1212159765914079234</td>\n",
       "      <td>TEEELAZER</td>\n",
       "      <td>1577836383</td>\n",
       "      <td>That $SPY $SPX puuump in the last hour was the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717960</th>\n",
       "      <td>1212159838882533376</td>\n",
       "      <td>ShortingIsFun</td>\n",
       "      <td>1577836401</td>\n",
       "      <td>In 2020 I may start Tweeting out positive news...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717961</th>\n",
       "      <td>1212160015332728833</td>\n",
       "      <td>Commuternyc</td>\n",
       "      <td>1577836443</td>\n",
       "      <td>Patiently Waiting for the no twitter sitter tw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717962</th>\n",
       "      <td>1212160410692046849</td>\n",
       "      <td>MoriaCrypto</td>\n",
       "      <td>1577836537</td>\n",
       "      <td>I don't discriminate. I own both $aapl and $ms...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717963</th>\n",
       "      <td>1212160477159206912</td>\n",
       "      <td>treabase</td>\n",
       "      <td>1577836553</td>\n",
       "      <td>$AAPL #patent 10,522,475 Vertical interconnect...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3717964 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tweet_id           writer   post_date  \\\n",
       "0         550441509175443456  VisualStockRSRC  1420070457   \n",
       "1         550441672312512512      KeralaGuy77  1420070496   \n",
       "2         550441732014223360      DozenStocks  1420070510   \n",
       "3         550442977802207232     ShowDreamCar  1420070807   \n",
       "4         550443807834402816     i_Know_First  1420071005   \n",
       "...                      ...              ...         ...   \n",
       "3717959  1212159765914079234        TEEELAZER  1577836383   \n",
       "3717960  1212159838882533376    ShortingIsFun  1577836401   \n",
       "3717961  1212160015332728833      Commuternyc  1577836443   \n",
       "3717962  1212160410692046849      MoriaCrypto  1577836537   \n",
       "3717963  1212160477159206912         treabase  1577836553   \n",
       "\n",
       "                                                      body  comment_num  \\\n",
       "0        lx21 made $10,008  on $AAPL -Check it out! htt...            0   \n",
       "1        Insanity of today weirdo massive selling. $aap...            0   \n",
       "2        S&P100 #Stocks Performance $HD $LOW $SBUX $TGT...            0   \n",
       "3        $GM $TSLA: Volkswagen Pushes 2014 Record Recal...            0   \n",
       "4        Swing Trading: Up To 8.91% Return In 14 Days h...            0   \n",
       "...                                                    ...          ...   \n",
       "3717959  That $SPY $SPX puuump in the last hour was the...            1   \n",
       "3717960  In 2020 I may start Tweeting out positive news...            0   \n",
       "3717961  Patiently Waiting for the no twitter sitter tw...            0   \n",
       "3717962  I don't discriminate. I own both $aapl and $ms...            1   \n",
       "3717963  $AAPL #patent 10,522,475 Vertical interconnect...            0   \n",
       "\n",
       "         retweet_num  like_num  \n",
       "0                  0         1  \n",
       "1                  0         0  \n",
       "2                  0         0  \n",
       "3                  0         1  \n",
       "4                  0         1  \n",
       "...              ...       ...  \n",
       "3717959            0         6  \n",
       "3717960            0         1  \n",
       "3717961            0         5  \n",
       "3717962            0         1  \n",
       "3717963            0         0  \n",
       "\n",
       "[3717964 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860c11c",
   "metadata": {},
   "source": [
    "### Cleaning up the Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9924d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df['raw_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6687c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_substring(string, pattern, replacement=''):\n",
    "    \n",
    "    substrings_to_remove = re.findall(pattern, string)\n",
    "    for substring in substrings_to_remove:\n",
    "        string = string.replace(substring, replacement)\n",
    "        \n",
    "    return string\n",
    "\n",
    "def reduce_repeated_chars(string):\n",
    "    new_string = ''\n",
    "    i = 0\n",
    "    while i < len(string):\n",
    "        j = i + 1\n",
    "        while j < len(string) and string[j] == string[i]:\n",
    "            j += 1\n",
    "        new_string += string[i] + (string[i] if j - i >= 2 else '')\n",
    "        i = j\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05411371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making everything lowercase\n",
    "df['body'] = df['body'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ba393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3708f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing patterns\n",
    "\n",
    "website_pattern = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)'\n",
    "numbers = '\\d+'\n",
    "usernames = '@[^\\s]+'\n",
    "tickers = '\\$[^\\s]+'\n",
    "extra_spaces = '  +'\n",
    "hashtags = '\\$[^\\s]+'\n",
    "next_lines = '\\\\n'\n",
    "\n",
    "\n",
    "df['body'] = df['body'].apply(lambda x: remove_substring(x, website_pattern))\n",
    "df['body'] = df['body'].apply(lambda x: x.encode('ascii', 'ignore').decode()) #emojis\n",
    "df['body'] = df['body'].apply(lambda x: remove_substring(x, usernames))\n",
    "df['body'] = df['body'].apply(lambda x: remove_substring(x, tickers))\n",
    "df['body'] = df['body'].apply(lambda x: reduce_repeated_chars(x))\n",
    "df['body'] = df['body'].apply(lambda x: remove_substring(x, numbers))\n",
    "df['body'] = df['body'].apply(lambda x: remove_substring(x, hashtags))\n",
    "df['body'] = df['body'].apply(lambda x: remove_substring(x, next_lines, ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "65d0cab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~‘’“”1234567890…ðŸ‘‰ðŸ‘ŒðŸ’¦âœ¨✰♡*•˛❤•\n"
     ]
    }
   ],
   "source": [
    "# removing all punctuation\n",
    "\n",
    "punct = punctuation + '‘’“”1234567890…ðŸ‘‰ðŸ‘ŒðŸ’¦âœ¨✰♡*•˛❤•'\n",
    "print(punct)\n",
    "df['body'] = df['body'].apply(lambda x: ''.join([c for c in x if c not in punct]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f52e7a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words \n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', \n",
    "             'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \n",
    "             'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', \n",
    "             'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', \n",
    "             'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \n",
    "             'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
    "             'after', 'to', 'from', 'in', 'out', 'on', 'off', 'again', 'further', 'then', 'once', 'here', 'there', \n",
    "             'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', \n",
    "             'such', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', \n",
    "             'don', 'dont', 'should', 'shouldve', 'now', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', \n",
    "             'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', \n",
    "             'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', \n",
    "             'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt', 'v', 'rn', 'lt', 'y', 'g', 'w', \n",
    "             'wk', 'sp', 'em', 'r', 'vs', 'd', 'ai', 't', 'mm', 'st', 'gt', 'n', 'id', 'p', 'f', 'm', 'b', 'c', \n",
    "             'pe', 'th', 'q', 'x', 'fb', 'ah', 'ill', 'u', 'oh', 'er', 'k', 's', 'im']\n",
    "\n",
    "df['body'] = df['body'].apply(lambda x: ' '.join(x for x in x.split() if x not in stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fe282d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finishing up by removing extra spaces\n",
    "df['body'] = df['body'].apply(lambda x: remove_substring(x, extra_spaces, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "437e8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing rows with just white spaces\n",
    "df = df[~(df['body'].str.contains('^\\s$', regex=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "22ee82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any rows with just one word wont provide enough context, so we'll remove them as well\n",
    "df = df[(df['body'].str.contains(' '))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ec035d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>raw_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-15T14:40:00Z</td>\n",
       "      <td>moving fast early</td>\n",
       "      <td>1</td>\n",
       "      <td>$AAPL Moving fast early!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-15T14:39:57Z</td>\n",
       "      <td>if confirms daily can rip</td>\n",
       "      <td>1</td>\n",
       "      <td>$AAPL if confirms daily can rip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-15T14:39:42Z</td>\n",
       "      <td>word got outdont miss your chance</td>\n",
       "      <td>1</td>\n",
       "      <td>$AAPL word got out .... don’t miss your chance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-15T14:39:40Z</td>\n",
       "      <td>who sold there weeklies to early lol</td>\n",
       "      <td>1</td>\n",
       "      <td>$AAPL who sold there weeklies to early lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-15T14:39:36Z</td>\n",
       "      <td>another walk atlets see if it gets gobbled up...</td>\n",
       "      <td>1</td>\n",
       "      <td>$AAPL another walk at 126 let’s see if it gets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             created_at                                               body  \\\n",
       "0  2020-12-15T14:40:00Z                                  moving fast early   \n",
       "1  2020-12-15T14:39:57Z                          if confirms daily can rip   \n",
       "2  2020-12-15T14:39:42Z                  word got outdont miss your chance   \n",
       "3  2020-12-15T14:39:40Z               who sold there weeklies to early lol   \n",
       "4  2020-12-15T14:39:36Z   another walk atlets see if it gets gobbled up...   \n",
       "\n",
       "   sentiment                                        raw_content  \n",
       "0          1                           $AAPL Moving fast early!  \n",
       "1          1                    $AAPL if confirms daily can rip  \n",
       "2          1     $AAPL word got out .... don’t miss your chance  \n",
       "3          1         $AAPL who sold there weeklies to early lol  \n",
       "4          1  $AAPL another walk at 126 let’s see if it gets...  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "66699b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take ~24 hours\n",
    "# # correct spellings\n",
    "\n",
    "# def spelling_check(text):\n",
    "#     global idx\n",
    "    \n",
    "#     idx += 1\n",
    "    \n",
    "#     if idx % 1e3 == 0: print(idx)\n",
    "        \n",
    "#     try:\n",
    "#         result = str(TextBlob(text).correct())\n",
    "#     except:\n",
    "#         print(f'failed at {idx}')\n",
    "        \n",
    "#     return result\n",
    "\n",
    "# idx = 0\n",
    "# df['body'] = df['body'].apply(lambda x: spelling_check(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1542ffbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928816, 4)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsetting a random sample of positive and negative to create a balanced dataset\n",
    "\n",
    "df_negative = df[df['sentiment'] == 0]\n",
    "df_positive = df.query(\"sentiment == 1\").sample(n=len(df_negative))\n",
    "\n",
    "df = pd.concat([df_negative, df_positive])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5f86dd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>raw_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-12-15T14:38:18Z</td>\n",
       "      <td>going right throughsupport as if it isnt even...</td>\n",
       "      <td>0</td>\n",
       "      <td>$MSFT Going right through 214 support as if it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-12-15T14:23:04Z</td>\n",
       "      <td>nobody gonna buy expensive ass iphones when t...</td>\n",
       "      <td>0</td>\n",
       "      <td>$AAPL nobody gonna buy expensive ass iPhones w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2020-12-15T14:12:10Z</td>\n",
       "      <td>robinhood peeps gonna be severely disappointe...</td>\n",
       "      <td>0</td>\n",
       "      <td>$AAPL Robinhood peeps gonna be severely disapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2020-12-15T13:33:52Z</td>\n",
       "      <td>always dump dump dump</td>\n",
       "      <td>0</td>\n",
       "      <td>$AAPL always dump dump dump.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2020-12-15T13:30:10Z</td>\n",
       "      <td>why is this turd not going anywhere this is p...</td>\n",
       "      <td>0</td>\n",
       "      <td>$AAPL why is this turd not going anywhere. Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593006</th>\n",
       "      <td>2022-01-28T15:12:17Z</td>\n",
       "      <td>soar baby soar</td>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA soar baby soar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116754</th>\n",
       "      <td>2021-11-09T15:28:11Z</td>\n",
       "      <td>evs getting decimated did brandon shit his pan...</td>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA $LCID EV&amp;#39;s getting decimated. Did Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911649</th>\n",
       "      <td>2020-02-26T13:57:07Z</td>\n",
       "      <td>apparently bears have short term memory</td>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA Apparently bears have short term memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899134</th>\n",
       "      <td>2022-02-23T18:38:07Z</td>\n",
       "      <td>holy shit i bought more callsmins ago and im ...</td>\n",
       "      <td>1</td>\n",
       "      <td>$SPY holy shit I bought more calls 5 mins ago ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926539</th>\n",
       "      <td>2017-06-09T19:22:12Z</td>\n",
       "      <td>chicken run today fortunately i kept it up an...</td>\n",
       "      <td>1</td>\n",
       "      <td>$NVDA chicken run today, fortunately I kept it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928816 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   created_at  \\\n",
       "14       2020-12-15T14:38:18Z   \n",
       "49       2020-12-15T14:23:04Z   \n",
       "61       2020-12-15T14:12:10Z   \n",
       "103      2020-12-15T13:33:52Z   \n",
       "106      2020-12-15T13:30:10Z   \n",
       "...                       ...   \n",
       "1593006  2022-01-28T15:12:17Z   \n",
       "1116754  2021-11-09T15:28:11Z   \n",
       "1911649  2020-02-26T13:57:07Z   \n",
       "1899134  2022-02-23T18:38:07Z   \n",
       "926539   2017-06-09T19:22:12Z   \n",
       "\n",
       "                                                      body  sentiment  \\\n",
       "14        going right throughsupport as if it isnt even...          0   \n",
       "49        nobody gonna buy expensive ass iphones when t...          0   \n",
       "61        robinhood peeps gonna be severely disappointe...          0   \n",
       "103                                  always dump dump dump          0   \n",
       "106       why is this turd not going anywhere this is p...          0   \n",
       "...                                                    ...        ...   \n",
       "1593006                                     soar baby soar          1   \n",
       "1116754  evs getting decimated did brandon shit his pan...          1   \n",
       "1911649            apparently bears have short term memory          1   \n",
       "1899134   holy shit i bought more callsmins ago and im ...          1   \n",
       "926539    chicken run today fortunately i kept it up an...          1   \n",
       "\n",
       "                                               raw_content  \n",
       "14       $MSFT Going right through 214 support as if it...  \n",
       "49       $AAPL nobody gonna buy expensive ass iPhones w...  \n",
       "61       $AAPL Robinhood peeps gonna be severely disapp...  \n",
       "103                           $AAPL always dump dump dump.  \n",
       "106      $AAPL why is this turd not going anywhere. Thi...  \n",
       "...                                                    ...  \n",
       "1593006                               $TSLA soar baby soar  \n",
       "1116754  $TSLA $LCID EV&#39;s getting decimated. Did Br...  \n",
       "1911649      $TSLA Apparently bears have short term memory  \n",
       "1899134  $SPY holy shit I bought more calls 5 mins ago ...  \n",
       "926539   $NVDA chicken run today, fortunately I kept it...  \n",
       "\n",
       "[928816 rows x 4 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatizing words\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "df['body'] = df['body'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "403922f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257978b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>raw_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-12-15T14:38:18Z</td>\n",
       "      <td>going right throughsupport as if it isnt even...</td>\n",
       "      <td>0</td>\n",
       "      <td>$MSFT Going right through 214 support as if it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-12-15T14:23:04Z</td>\n",
       "      <td>nobody gonna buy expensive ass iphones when t...</td>\n",
       "      <td>0</td>\n",
       "      <td>$AAPL nobody gonna buy expensive ass iPhones w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2020-12-15T14:12:10Z</td>\n",
       "      <td>robinhood peeps gonna be severely disappointe...</td>\n",
       "      <td>0</td>\n",
       "      <td>$AAPL Robinhood peeps gonna be severely disapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2020-12-15T13:33:52Z</td>\n",
       "      <td>always dump dump dump</td>\n",
       "      <td>0</td>\n",
       "      <td>$AAPL always dump dump dump.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2020-12-15T13:30:10Z</td>\n",
       "      <td>why is this turd not going anywhere this is p...</td>\n",
       "      <td>0</td>\n",
       "      <td>$AAPL why is this turd not going anywhere. Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593006</th>\n",
       "      <td>2022-01-28T15:12:17Z</td>\n",
       "      <td>soar baby soar</td>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA soar baby soar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116754</th>\n",
       "      <td>2021-11-09T15:28:11Z</td>\n",
       "      <td>evs getting decimated did brandon shit his pan...</td>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA $LCID EV&amp;#39;s getting decimated. Did Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911649</th>\n",
       "      <td>2020-02-26T13:57:07Z</td>\n",
       "      <td>apparently bears have short term memory</td>\n",
       "      <td>1</td>\n",
       "      <td>$TSLA Apparently bears have short term memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899134</th>\n",
       "      <td>2022-02-23T18:38:07Z</td>\n",
       "      <td>holy shit i bought more callsmins ago and im ...</td>\n",
       "      <td>1</td>\n",
       "      <td>$SPY holy shit I bought more calls 5 mins ago ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926539</th>\n",
       "      <td>2017-06-09T19:22:12Z</td>\n",
       "      <td>chicken run today fortunately i kept it up an...</td>\n",
       "      <td>1</td>\n",
       "      <td>$NVDA chicken run today, fortunately I kept it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928816 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   created_at  \\\n",
       "14       2020-12-15T14:38:18Z   \n",
       "49       2020-12-15T14:23:04Z   \n",
       "61       2020-12-15T14:12:10Z   \n",
       "103      2020-12-15T13:33:52Z   \n",
       "106      2020-12-15T13:30:10Z   \n",
       "...                       ...   \n",
       "1593006  2022-01-28T15:12:17Z   \n",
       "1116754  2021-11-09T15:28:11Z   \n",
       "1911649  2020-02-26T13:57:07Z   \n",
       "1899134  2022-02-23T18:38:07Z   \n",
       "926539   2017-06-09T19:22:12Z   \n",
       "\n",
       "                                                      body  sentiment  \\\n",
       "14        going right throughsupport as if it isnt even...          0   \n",
       "49        nobody gonna buy expensive ass iphones when t...          0   \n",
       "61        robinhood peeps gonna be severely disappointe...          0   \n",
       "103                                  always dump dump dump          0   \n",
       "106       why is this turd not going anywhere this is p...          0   \n",
       "...                                                    ...        ...   \n",
       "1593006                                     soar baby soar          1   \n",
       "1116754  evs getting decimated did brandon shit his pan...          1   \n",
       "1911649            apparently bears have short term memory          1   \n",
       "1899134   holy shit i bought more callsmins ago and im ...          1   \n",
       "926539    chicken run today fortunately i kept it up an...          1   \n",
       "\n",
       "                                               raw_content  \n",
       "14       $MSFT Going right through 214 support as if it...  \n",
       "49       $AAPL nobody gonna buy expensive ass iPhones w...  \n",
       "61       $AAPL Robinhood peeps gonna be severely disapp...  \n",
       "103                           $AAPL always dump dump dump.  \n",
       "106      $AAPL why is this turd not going anywhere. Thi...  \n",
       "...                                                    ...  \n",
       "1593006                               $TSLA soar baby soar  \n",
       "1116754  $TSLA $LCID EV&#39;s getting decimated. Did Br...  \n",
       "1911649      $TSLA Apparently bears have short term memory  \n",
       "1899134  $SPY holy shit I bought more calls 5 mins ago ...  \n",
       "926539   $NVDA chicken run today, fortunately I kept it...  \n",
       "\n",
       "[928816 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./data/preprocessing/balanced_untokenized_cleaned_stocktwits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2c65ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/balanced_untokenized_cleaned_stocktwits.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003e5e4",
   "metadata": {},
   "source": [
    "### Lemmatization + Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "257286b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize words\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "df['body'] = df['body'].apply(lambda x: w_tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "216d072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/balanced_tokenized_cleaned_stocktwits.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d19a02",
   "metadata": {},
   "source": [
    "### Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0da991d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "corpus = df['body'].values\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "763c70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an int-mapping dictionary\n",
    "vocab_to_int = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e606e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "padded_sequences = pad_sequences(sequences, 31, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c1788ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(padded_sequences)\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "18515747",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4ca569c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('./data/padded_X.csv', index_label=False)\n",
    "y.to_csv('./data/padded_y.csv', index_label=False)\n",
    "pd.DataFrame([vocab_to_int]).to_csv('./data/vocab_words.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58ee6d",
   "metadata": {},
   "source": [
    "### Create Preprocessing Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "581dbd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "dct = pd.read_csv('./data/vocab_words.csv').to_dict(orient='records')[0]\n",
    "\n",
    "websites = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)'\n",
    "numbers = '\\d+'\n",
    "usernames = '@[^\\s]+'\n",
    "tickers = '\\$[^\\s]+'\n",
    "extra_spaces = '  +'\n",
    "hashtags = '\\$[^\\s]+'\n",
    "next_lines = '\\\\n'\n",
    "\n",
    "punctuation = \"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~‘’“”1234567890…ðŸ‘‰ðŸ‘ŒðŸ’¦âœ¨✰♡*•˛❤•\" + '\"'\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', \n",
    "             'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \n",
    "             'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', \n",
    "             'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', \n",
    "             'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "             'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \n",
    "             'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
    "             'after', 'to', 'from', 'in', 'out', 'on', 'off', 'again', 'further', 'then', 'once', 'here', 'there', \n",
    "             'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', \n",
    "             'such', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', \n",
    "             'don', 'dont', 'should', 'shouldve', 'now', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', \n",
    "             'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', \n",
    "             'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', \n",
    "             'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt', 'v', 'rn', 'lt', 'y', 'g', 'w', \n",
    "             'wk', 'sp', 'em', 'r', 'vs', 'd', 'ai', 't', 'mm', 'st', 'gt', 'n', 'id', 'p', 'f', 'm', 'b', 'c', \n",
    "             'pe', 'th', 'q', 'x', 'fb', 'ah', 'ill', 'u', 'oh', 'er', 'k', 's', 'im']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e995be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    '''\n",
    "    preprocess the text to input into model\n",
    "    '''\n",
    "    \n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # make texts lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove websites and usernames, if exist\n",
    "    text = re.sub(websites, '', text)\n",
    "    text = re.sub(usernames, '', text)\n",
    "    text = re.sub(numbers, '', text)\n",
    "    text = re.sub(tickers, '', text)\n",
    "    text = re.sub(hashtags, '', text)\n",
    "    text = re.sub(next_lines, '', text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = ''.join([x for x in text if x not in punctuation])\n",
    "    \n",
    "    # remove additional characters down to 2\n",
    "    text = re.sub(re.compile(r'(\\w)\\1+'), r'\\1\\1', text)\n",
    "    \n",
    "    # remove stop words\n",
    "    text = ' '.join(text.lower() for text in text.split() if text not in stopwords)\n",
    "    \n",
    "    # remove additional spaces\n",
    "    text = re.sub(extra_spaces, '', text)\n",
    "    \n",
    "    # lemmatize & tokenize\n",
    "    text = [lemmatizer.lemmatize(x) for x in w_tokenizer.tokenize(text)]\n",
    "    \n",
    "    # numericalize\n",
    "    text_int = []\n",
    "    text_int.append([dct[word] for word in text])\n",
    "    \n",
    "    return text_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14e64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc56f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "307dc297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1119,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'hello'\n",
    "pad_sequences(preprocess(text), 31, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fcec41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
